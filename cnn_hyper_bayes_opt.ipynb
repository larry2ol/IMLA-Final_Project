{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac69124d-9a54-427d-a58e-386937fb2e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import collections\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import random\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import cv2\n",
    "import collections\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3eaf8e6-1daf-4598-8c10-977728764aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.ToTensor(),         \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3970c5c6-192b-40c1-9a89-718105ab417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd_path = pathlib.Path(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a01750eb-60b0-4b66-ad00-097b2d0fe773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Lenovo\\\\Videos\\\\Imperial college\\\\week 25\\\\Project2\\\\data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "img_dir = os.path.join(cwd_path, 'data')\n",
    "img_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44de1b69-bed1-48f3-8ab2-03594a8b0cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load image data\n",
    "def load_images(data_dir, transform= transform):\n",
    "    dataset = datasets.ImageFolder(data_dir, transform)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = int(0.1 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "    sample = train_dataset[0][0]\n",
    "    samplePilImg = transforms.ToPILImage()(sample)\n",
    "    samplePilImg.show()\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b35965af-0a78-48b3-8248-ceb53f41c2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "780"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset = load_images(img_dir, transform)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c40fbf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({0: 434, 1: 346})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=[y for _,y in train_dataset]\n",
    "counter_train=collections.Counter(y_train)\n",
    "counter_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0223343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 52, 1: 45})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val=[y for _,y in val_dataset]\n",
    "counter_val=collections.Counter(y_val)\n",
    "counter_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d8b3e6b-60a7-4932-b9cd-c2cd68c30fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 56, 1: 43})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test=[y for _,y in test_dataset]\n",
    "counter_test=collections.Counter(y_test)\n",
    "counter_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5642f763-52f0-41a2-95eb-bd61515031ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ccf1e89-8d30-48f9-a368-1c8ac574b2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 224, 224])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "img , label = next(iter(train_loader))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f97411d-eee2-47ae-bca3-0e87ee8076b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayesian_optimization\n",
      "  Downloading bayesian_optimization-1.5.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.6 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from bayesian_optimization) (0.4.6)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bayesian_optimization) (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.25 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bayesian_optimization) (1.26.4)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bayesian_optimization) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian_optimization) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian_optimization) (3.5.0)\n",
      "Installing collected packages: bayesian-optimization\n",
      "Successfully installed bayesian-optimization-1.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "|   iter    |  target   | conv_l... |  dropout  | dropou... | dropou... | dropou... | kernel... |  kernels  |  layers   | maxpoo... |  neurons  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Calling CNNModel with kwargs: {'kernels': 16, 'kernel_size': 3, 'conv_layers': 1, 'maxpooling': 0, 'dropout_cnn': 0, 'dropout_perc_cnn': 0.15870235632684523, 'layers': 1, 'neurons': 298, 'dropout': 0, 'dropout_perc': 0.22093302905273593}\n",
      "Initializing CNNModel with kwargs: {'kernels': 16, 'kernel_size': 3, 'conv_layers': 1, 'maxpooling': 0, 'dropout_cnn': 0, 'dropout_perc_cnn': 0.15870235632684523, 'layers': 1, 'neurons': 298, 'dropout': 0, 'dropout_perc': 0.22093302905273593}\n",
      "Epoch 1, Batch 1, Loss: 0.6946\n",
      "Epoch 1, Batch 2, Loss: 37.0538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 3, Loss: 40.9049\n",
      "Epoch 1, Batch 4, Loss: 32.8125\n",
      "Epoch 1, Batch 5, Loss: 43.7500\n",
      "Epoch 1, Batch 6, Loss: 25.4017\n",
      "Epoch 1, Batch 7, Loss: 43.7500\n",
      "Epoch 1, Batch 8, Loss: 40.6250\n",
      "Epoch 1, Batch 9, Loss: 35.9375\n",
      "Epoch 1, Batch 10, Loss: 29.6875\n",
      "Epoch 1, Batch 11, Loss: 42.1875\n",
      "Epoch 1, Batch 12, Loss: 26.5625\n",
      "Epoch 1, Batch 13, Loss: 33.3333\n",
      "Epoch 2, Batch 1, Loss: 25.9806\n",
      "Epoch 2, Batch 2, Loss: 39.0625\n",
      "Epoch 2, Batch 3, Loss: 42.1875\n",
      "Epoch 2, Batch 4, Loss: 23.4375\n",
      "Epoch 2, Batch 5, Loss: 34.3750\n",
      "Epoch 2, Batch 6, Loss: 42.1875\n",
      "Epoch 2, Batch 7, Loss: 34.3750\n",
      "Epoch 2, Batch 8, Loss: 41.2009\n",
      "Epoch 2, Batch 9, Loss: 42.1875\n",
      "Epoch 2, Batch 10, Loss: 35.9375\n",
      "Epoch 2, Batch 11, Loss: 32.8125\n",
      "Epoch 2, Batch 12, Loss: 35.9375\n",
      "Epoch 2, Batch 13, Loss: 16.6667\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m2.792e+03\u001b[39m | \u001b[39m1.834    \u001b[39m | \u001b[39m0.7203   \u001b[39m | \u001b[39m0.0001144\u001b[39m | \u001b[39m0.2209   \u001b[39m | \u001b[39m0.1587   \u001b[39m | \u001b[39m3.185    \u001b[39m | \u001b[39m16.8     \u001b[39m | \u001b[39m1.691    \u001b[39m | \u001b[39m0.3968   \u001b[39m | \u001b[39m298.9    \u001b[39m |\n",
      "Calling CNNModel with kwargs: {'kernels': 30, 'kernel_size': 4, 'conv_layers': 1, 'maxpooling': 0, 'dropout_cnn': 0, 'dropout_perc_cnn': 0.11095503727917047, 'layers': 2, 'neurons': 150, 'dropout': 0, 'dropout_perc': 0.4512469745563782}\n",
      "Initializing CNNModel with kwargs: {'kernels': 30, 'kernel_size': 4, 'conv_layers': 1, 'maxpooling': 0, 'dropout_cnn': 0, 'dropout_perc_cnn': 0.11095503727917047, 'layers': 2, 'neurons': 150, 'dropout': 0, 'dropout_perc': 0.4512469745563782}\n",
      "Epoch 1, Batch 1, Loss: 0.6903\n",
      "Epoch 1, Batch 2, Loss: 42.0062\n",
      "Epoch 1, Batch 3, Loss: 29.1072\n",
      "Epoch 1, Batch 4, Loss: 35.8505\n",
      "Epoch 1, Batch 5, Loss: 49.5022\n",
      "Epoch 1, Batch 6, Loss: 51.8476\n",
      "Epoch 1, Batch 7, Loss: 40.2217\n",
      "Epoch 1, Batch 8, Loss: 32.0329\n",
      "Epoch 1, Batch 9, Loss: 35.5158\n",
      "Epoch 1, Batch 10, Loss: 35.9375\n",
      "Epoch 1, Batch 11, Loss: 32.5151\n",
      "Epoch 1, Batch 12, Loss: 48.4375\n",
      "Epoch 1, Batch 13, Loss: 33.3333\n",
      "Epoch 2, Batch 1, Loss: 25.0000\n",
      "Epoch 2, Batch 2, Loss: 29.6875\n",
      "Epoch 2, Batch 3, Loss: 31.2500\n",
      "Epoch 2, Batch 4, Loss: 42.1875\n",
      "Epoch 2, Batch 5, Loss: 34.3750\n",
      "Epoch 2, Batch 6, Loss: 31.2500\n",
      "Epoch 2, Batch 7, Loss: 39.0625\n",
      "Epoch 2, Batch 8, Loss: 32.8125\n",
      "Epoch 2, Batch 9, Loss: 31.2500\n",
      "Epoch 2, Batch 10, Loss: 42.1875\n",
      "Epoch 2, Batch 11, Loss: 48.4375\n",
      "Epoch 2, Batch 12, Loss: 43.7500\n",
      "Epoch 2, Batch 13, Loss: 16.6667\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m2.769e+03\u001b[39m | \u001b[39m1.838    \u001b[39m | \u001b[39m0.6852   \u001b[39m | \u001b[39m0.2045   \u001b[39m | \u001b[39m0.4512   \u001b[39m | \u001b[39m0.111    \u001b[39m | \u001b[39m4.341    \u001b[39m | \u001b[39m30.2     \u001b[39m | \u001b[39m2.117    \u001b[39m | \u001b[39m0.1404   \u001b[39m | \u001b[39m150.4    \u001b[39m |\n",
      "Calling CNNModel with kwargs: {'kernels': 10, 'kernel_size': 4, 'conv_layers': 2, 'maxpooling': 0, 'dropout_cnn': 0, 'dropout_perc_cnn': 0.45055566091841537, 'layers': 1, 'neurons': 446, 'dropout': 0, 'dropout_perc': 0.3769290462677256}\n",
      "Initializing CNNModel with kwargs: {'kernels': 10, 'kernel_size': 4, 'conv_layers': 2, 'maxpooling': 0, 'dropout_cnn': 0, 'dropout_perc_cnn': 0.45055566091841537, 'layers': 1, 'neurons': 446, 'dropout': 0, 'dropout_perc': 0.3769290462677256}\n",
      "Epoch 1, Batch 1, Loss: 0.6977\n",
      "Epoch 1, Batch 2, Loss: 39.2586\n",
      "Epoch 1, Batch 3, Loss: 42.5158\n",
      "Epoch 1, Batch 4, Loss: 40.6250\n",
      "Epoch 1, Batch 5, Loss: 40.6250\n",
      "Epoch 1, Batch 6, Loss: 39.5781\n",
      "Epoch 1, Batch 7, Loss: 36.9777\n",
      "Epoch 1, Batch 8, Loss: 42.6949\n",
      "Epoch 1, Batch 9, Loss: 39.8641\n",
      "Epoch 1, Batch 10, Loss: 40.7643\n",
      "Epoch 1, Batch 11, Loss: 40.6250\n",
      "Epoch 1, Batch 12, Loss: 37.5000\n",
      "Epoch 1, Batch 13, Loss: 8.3333\n",
      "Epoch 2, Batch 1, Loss: 35.6546\n",
      "Epoch 2, Batch 2, Loss: 29.7063\n",
      "Epoch 2, Batch 3, Loss: 31.2500\n",
      "Epoch 2, Batch 4, Loss: 39.4168\n",
      "Epoch 2, Batch 5, Loss: 42.7018\n",
      "Epoch 2, Batch 6, Loss: 32.2524\n",
      "Epoch 2, Batch 7, Loss: 45.3125\n",
      "Epoch 2, Batch 8, Loss: 39.0625\n",
      "Epoch 2, Batch 9, Loss: 45.7281\n",
      "Epoch 2, Batch 10, Loss: 48.4897\n",
      "Epoch 2, Batch 11, Loss: 32.8125\n",
      "Epoch 2, Batch 12, Loss: 41.2411\n",
      "Epoch 2, Batch 13, Loss: 41.6667\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m2.627e+03\u001b[39m | \u001b[39m2.601    \u001b[39m | \u001b[39m0.9683   \u001b[39m | \u001b[39m0.3134   \u001b[39m | \u001b[39m0.3769   \u001b[39m | \u001b[39m0.4506   \u001b[39m | \u001b[39m4.789    \u001b[39m | \u001b[39m10.93    \u001b[39m | \u001b[39m1.078    \u001b[39m | \u001b[39m0.1698   \u001b[39m | \u001b[39m446.9    \u001b[39m |\n",
      "Calling CNNModel with kwargs: {'kernels': 45, 'kernel_size': 3, 'conv_layers': 1, 'maxpooling': 0, 'dropout_cnn': 0, 'dropout_perc_cnn': 0.37675084558018934, 'layers': 2, 'neurons': 391, 'dropout': 0, 'dropout_perc': 0.31326611398920684}\n",
      "Initializing CNNModel with kwargs: {'kernels': 45, 'kernel_size': 3, 'conv_layers': 1, 'maxpooling': 0, 'dropout_cnn': 0, 'dropout_perc_cnn': 0.37675084558018934, 'layers': 2, 'neurons': 391, 'dropout': 0, 'dropout_perc': 0.31326611398920684}\n",
      "Epoch 1, Batch 1, Loss: 0.6911\n",
      "Epoch 1, Batch 2, Loss: 36.5600\n",
      "Epoch 1, Batch 3, Loss: 47.4838\n",
      "Epoch 1, Batch 4, Loss: 48.4375\n",
      "Epoch 1, Batch 5, Loss: 43.6601\n",
      "Epoch 1, Batch 6, Loss: 42.1875\n",
      "Epoch 1, Batch 7, Loss: 43.7500\n",
      "Epoch 1, Batch 8, Loss: 32.8125\n",
      "Epoch 1, Batch 9, Loss: 42.1875\n",
      "Epoch 1, Batch 10, Loss: 43.7500\n",
      "Epoch 1, Batch 11, Loss: 32.8125\n",
      "Epoch 1, Batch 12, Loss: 42.1875\n",
      "Epoch 1, Batch 13, Loss: 41.6667\n",
      "Epoch 2, Batch 1, Loss: 43.7500\n",
      "Epoch 2, Batch 2, Loss: 42.1875\n",
      "Epoch 2, Batch 3, Loss: 42.1875\n",
      "Epoch 2, Batch 4, Loss: 34.3750\n",
      "Epoch 2, Batch 5, Loss: 46.8750\n",
      "Epoch 2, Batch 6, Loss: 42.1875\n",
      "Epoch 2, Batch 7, Loss: 40.6250\n",
      "Epoch 2, Batch 8, Loss: 40.6250\n",
      "Epoch 2, Batch 9, Loss: 39.0625\n",
      "Epoch 2, Batch 10, Loss: 40.6250\n",
      "Epoch 2, Batch 11, Loss: 39.0625\n",
      "Epoch 2, Batch 12, Loss: 29.6875\n",
      "Epoch 2, Batch 13, Loss: 41.6667\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m2.742e+03\u001b[39m | \u001b[39m1.197    \u001b[39m | \u001b[39m0.4211   \u001b[39m | \u001b[39m0.9579   \u001b[39m | \u001b[39m0.3133   \u001b[39m | \u001b[39m0.3768   \u001b[39m | \u001b[39m3.631    \u001b[39m | \u001b[39m45.82    \u001b[39m | \u001b[39m2.669    \u001b[39m | \u001b[39m0.01829  \u001b[39m | \u001b[39m391.1    \u001b[39m |\n",
      "Calling CNNModel with kwargs: {'kernels': 58, 'kernel_size': 3, 'conv_layers': 2, 'maxpooling': 0, 'dropout_cnn': 0, 'dropout_perc_cnn': 0.14129040263105683, 'layers': 1, 'neurons': 120, 'dropout': 0, 'dropout_perc': 0.4157117313805955}\n",
      "Initializing CNNModel with kwargs: {'kernels': 58, 'kernel_size': 3, 'conv_layers': 2, 'maxpooling': 0, 'dropout_cnn': 0, 'dropout_perc_cnn': 0.14129040263105683, 'layers': 1, 'neurons': 120, 'dropout': 0, 'dropout_perc': 0.4157117313805955}\n",
      "Epoch 1, Batch 1, Loss: 0.6921\n",
      "Epoch 1, Batch 2, Loss: 29.7329\n",
      "Epoch 1, Batch 3, Loss: 29.6144\n",
      "Epoch 1, Batch 4, Loss: 39.0626\n",
      "Epoch 1, Batch 5, Loss: 35.9375\n",
      "Epoch 1, Batch 6, Loss: 34.3750\n",
      "Epoch 1, Batch 7, Loss: 46.8750\n",
      "Epoch 1, Batch 8, Loss: 43.7500\n",
      "Epoch 1, Batch 9, Loss: 43.7500\n",
      "Epoch 1, Batch 10, Loss: 37.5000\n",
      "Epoch 1, Batch 11, Loss: 43.7500\n",
      "Epoch 1, Batch 12, Loss: 57.8125\n",
      "Epoch 1, Batch 13, Loss: 83.3333\n",
      "Epoch 2, Batch 1, Loss: 54.6875\n",
      "Epoch 2, Batch 2, Loss: 48.1969\n",
      "Epoch 2, Batch 3, Loss: 53.1250\n",
      "Epoch 2, Batch 4, Loss: 60.9375\n",
      "Epoch 2, Batch 5, Loss: 60.9375\n",
      "Epoch 2, Batch 6, Loss: 54.6875\n",
      "Epoch 2, Batch 7, Loss: 48.4375\n",
      "Epoch 2, Batch 8, Loss: 53.1250\n",
      "Epoch 2, Batch 9, Loss: 50.0000\n",
      "Epoch 2, Batch 10, Loss: 45.3125\n",
      "Epoch 2, Batch 11, Loss: 53.1250\n",
      "Epoch 2, Batch 12, Loss: 37.5000\n",
      "Epoch 2, Batch 13, Loss: 25.0000\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m2.503e+03\u001b[39m | \u001b[39m2.978    \u001b[39m | \u001b[39m0.7482   \u001b[39m | \u001b[39m0.2804   \u001b[39m | \u001b[39m0.4157   \u001b[39m | \u001b[39m0.1413   \u001b[39m | \u001b[39m3.896    \u001b[39m | \u001b[39m58.7     \u001b[39m | \u001b[39m1.587    \u001b[39m | \u001b[39m0.2878   \u001b[39m | \u001b[39m120.7    \u001b[39m |\n",
      "Calling CNNModel with kwargs: {'kernels': 16, 'kernel_size': 3, 'conv_layers': 2, 'maxpooling': 0, 'dropout_cnn': 0, 'dropout_perc_cnn': 0.12673324992230403, 'layers': 1, 'neurons': 298, 'dropout': 0, 'dropout_perc': 0.3811236937722066}\n",
      "Initializing CNNModel with kwargs: {'kernels': 16, 'kernel_size': 3, 'conv_layers': 2, 'maxpooling': 0, 'dropout_cnn': 0, 'dropout_perc_cnn': 0.12673324992230403, 'layers': 1, 'neurons': 298, 'dropout': 0, 'dropout_perc': 0.3811236937722066}\n",
      "Epoch 1, Batch 1, Loss: 0.6954\n",
      "Epoch 1, Batch 2, Loss: 46.1448\n",
      "Epoch 1, Batch 3, Loss: 38.8388\n",
      "Epoch 1, Batch 4, Loss: 51.0445\n",
      "Epoch 1, Batch 5, Loss: 42.1875\n",
      "Epoch 1, Batch 6, Loss: 43.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 7, Loss: 37.6083\n",
      "Epoch 1, Batch 8, Loss: 43.7500\n",
      "Epoch 1, Batch 9, Loss: 45.3125\n",
      "Epoch 1, Batch 10, Loss: 31.2500\n",
      "Epoch 1, Batch 11, Loss: 43.3835\n",
      "Epoch 1, Batch 12, Loss: 48.4375\n",
      "Epoch 1, Batch 13, Loss: 16.6667\n",
      "Epoch 2, Batch 1, Loss: 45.3125\n",
      "Epoch 2, Batch 2, Loss: 42.1875\n",
      "Epoch 2, Batch 3, Loss: 32.9398\n",
      "Epoch 2, Batch 4, Loss: 43.4067\n",
      "Epoch 2, Batch 5, Loss: 38.6134\n",
      "Epoch 2, Batch 6, Loss: 40.6250\n",
      "Epoch 2, Batch 7, Loss: 42.1875\n",
      "Epoch 2, Batch 8, Loss: 42.1875\n",
      "Epoch 2, Batch 9, Loss: 38.6434\n",
      "Epoch 2, Batch 10, Loss: 43.9024\n",
      "Epoch 2, Batch 11, Loss: 54.7165\n",
      "Epoch 2, Batch 12, Loss: 50.0000\n",
      "Epoch 2, Batch 13, Loss: 41.6667\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m2.88e+03 \u001b[39m | \u001b[35m2.348    \u001b[39m | \u001b[35m0.8897   \u001b[39m | \u001b[35m0.4293   \u001b[39m | \u001b[35m0.3811   \u001b[39m | \u001b[35m0.1267   \u001b[39m | \u001b[35m3.372    \u001b[39m | \u001b[35m16.28    \u001b[39m | \u001b[35m1.078    \u001b[39m | \u001b[35m0.3834   \u001b[39m | \u001b[35m298.2    \u001b[39m |\n",
      "Calling CNNModel with kwargs: {'kernels': 15, 'kernel_size': 3, 'conv_layers': 3, 'maxpooling': 0, 'dropout_cnn': 1, 'dropout_perc_cnn': 0.1, 'layers': 1, 'neurons': 296, 'dropout': 1, 'dropout_perc': 0.5}\n",
      "Initializing CNNModel with kwargs: {'kernels': 15, 'kernel_size': 3, 'conv_layers': 3, 'maxpooling': 0, 'dropout_cnn': 1, 'dropout_perc_cnn': 0.1, 'layers': 1, 'neurons': 296, 'dropout': 1, 'dropout_perc': 0.5}\n",
      "Epoch 1, Batch 1, Loss: 0.7026\n",
      "Epoch 1, Batch 2, Loss: 34.9213\n",
      "Epoch 1, Batch 3, Loss: 31.4270\n",
      "Epoch 1, Batch 4, Loss: 31.4463\n",
      "Epoch 1, Batch 5, Loss: 24.2817\n",
      "Epoch 1, Batch 6, Loss: 32.4930\n",
      "Epoch 1, Batch 7, Loss: 45.3125\n",
      "Epoch 1, Batch 8, Loss: 31.0320\n",
      "Epoch 1, Batch 9, Loss: 33.7602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10, Loss: 43.3581\n",
      "Epoch 1, Batch 11, Loss: 36.8985\n",
      "Epoch 1, Batch 12, Loss: 33.1818\n",
      "Epoch 1, Batch 13, Loss: 16.6667\n",
      "Epoch 2, Batch 1, Loss: 33.1630\n",
      "Epoch 2, Batch 2, Loss: 40.6250\n",
      "Epoch 2, Batch 3, Loss: 37.5000\n",
      "Epoch 2, Batch 4, Loss: 37.5857\n",
      "Epoch 2, Batch 5, Loss: 43.1417\n",
      "Epoch 2, Batch 6, Loss: 39.0625\n",
      "Epoch 2, Batch 7, Loss: 43.3615\n",
      "Epoch 2, Batch 8, Loss: 31.4991\n",
      "Epoch 2, Batch 9, Loss: 33.4940\n",
      "Epoch 2, Batch 10, Loss: 41.5253\n",
      "Epoch 2, Batch 11, Loss: 23.4400\n",
      "Epoch 2, Batch 12, Loss: 36.0255\n",
      "Epoch 2, Batch 13, Loss: 25.0000\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m2.721e+03\u001b[39m | \u001b[39m3.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m3.8      \u001b[39m | \u001b[39m15.07    \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.3524   \u001b[39m | \u001b[39m296.4    \u001b[39m |\n",
      "Calling CNNModel with kwargs: {'kernels': 17, 'kernel_size': 3, 'conv_layers': 3, 'maxpooling': 0, 'dropout_cnn': 1, 'dropout_perc_cnn': 0.1, 'layers': 1, 'neurons': 297, 'dropout': 1, 'dropout_perc': 0.5}\n",
      "Initializing CNNModel with kwargs: {'kernels': 17, 'kernel_size': 3, 'conv_layers': 3, 'maxpooling': 0, 'dropout_cnn': 1, 'dropout_perc_cnn': 0.1, 'layers': 1, 'neurons': 297, 'dropout': 1, 'dropout_perc': 0.5}\n",
      "Epoch 1, Batch 1, Loss: 0.6875\n",
      "Epoch 1, Batch 2, Loss: 37.4464\n",
      "Epoch 1, Batch 3, Loss: 36.4518\n",
      "Epoch 1, Batch 4, Loss: 45.7729\n",
      "Epoch 1, Batch 5, Loss: 26.5646\n",
      "Epoch 1, Batch 6, Loss: 35.9375\n",
      "Epoch 1, Batch 7, Loss: 31.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 8, Loss: 44.4854\n",
      "Epoch 1, Batch 9, Loss: 39.0625\n",
      "Epoch 1, Batch 10, Loss: 34.3752\n",
      "Epoch 1, Batch 11, Loss: 35.9375\n",
      "Epoch 1, Batch 12, Loss: 46.8750\n",
      "Epoch 1, Batch 13, Loss: 16.6667\n",
      "Epoch 2, Batch 1, Loss: 32.8125\n",
      "Epoch 2, Batch 2, Loss: 43.7500\n",
      "Epoch 2, Batch 3, Loss: 40.6250\n",
      "Epoch 2, Batch 4, Loss: 32.8125\n",
      "Epoch 2, Batch 5, Loss: 42.1875\n",
      "Epoch 2, Batch 6, Loss: 31.2500\n",
      "Epoch 2, Batch 7, Loss: 43.7500\n",
      "Epoch 2, Batch 8, Loss: 39.5121\n",
      "Epoch 2, Batch 9, Loss: 31.2500\n",
      "Epoch 2, Batch 10, Loss: 39.0625\n",
      "Epoch 2, Batch 11, Loss: 39.0625\n",
      "Epoch 2, Batch 12, Loss: 34.3750\n",
      "Epoch 2, Batch 13, Loss: 50.0000\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m2.734e+03\u001b[39m | \u001b[39m3.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m3.08     \u001b[39m | \u001b[39m17.44    \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.7259   \u001b[39m | \u001b[39m297.9    \u001b[39m |\n",
      "Calling CNNModel with kwargs: {'kernels': 15, 'kernel_size': 3, 'conv_layers': 2, 'maxpooling': 0, 'dropout_cnn': 0, 'dropout_perc_cnn': 0.1, 'layers': 1, 'neurons': 298, 'dropout': 1, 'dropout_perc': 0.5}\n",
      "Initializing CNNModel with kwargs: {'kernels': 15, 'kernel_size': 3, 'conv_layers': 2, 'maxpooling': 0, 'dropout_cnn': 0, 'dropout_perc_cnn': 0.1, 'layers': 1, 'neurons': 298, 'dropout': 1, 'dropout_perc': 0.5}\n",
      "Epoch 1, Batch 1, Loss: 0.6868\n",
      "Epoch 1, Batch 2, Loss: 12.2096\n",
      "Epoch 1, Batch 3, Loss: 41.3590\n",
      "Epoch 1, Batch 4, Loss: 56.2500\n",
      "Epoch 1, Batch 5, Loss: 59.3750\n",
      "Epoch 1, Batch 6, Loss: 57.8125\n",
      "Epoch 1, Batch 7, Loss: 51.5625\n",
      "Epoch 1, Batch 8, Loss: 54.6875\n",
      "Epoch 1, Batch 9, Loss: 59.3750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10, Loss: 53.1250\n",
      "Epoch 1, Batch 11, Loss: 62.5000\n",
      "Epoch 1, Batch 12, Loss: 57.8125\n",
      "Epoch 1, Batch 13, Loss: 58.3333\n",
      "Epoch 2, Batch 1, Loss: 54.6875\n",
      "Epoch 2, Batch 2, Loss: 62.5000\n",
      "Epoch 2, Batch 3, Loss: 46.8750\n",
      "Epoch 2, Batch 4, Loss: 45.3125\n",
      "Epoch 2, Batch 5, Loss: 53.1250\n",
      "Epoch 2, Batch 6, Loss: 56.2500\n",
      "Epoch 2, Batch 7, Loss: 62.5000\n",
      "Epoch 2, Batch 8, Loss: 57.8125\n",
      "Epoch 2, Batch 9, Loss: 59.3750\n",
      "Epoch 2, Batch 10, Loss: 56.2500\n",
      "Epoch 2, Batch 11, Loss: 62.5000\n",
      "Epoch 2, Batch 12, Loss: 53.1250\n",
      "Epoch 2, Batch 13, Loss: 33.3333\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m2.554e+03\u001b[39m | \u001b[39m2.287    \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.3075   \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m3.573    \u001b[39m | \u001b[39m15.35    \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.03369  \u001b[39m | \u001b[39m298.8    \u001b[39m |\n",
      "Calling CNNModel with kwargs: {'kernels': 16, 'kernel_size': 3, 'conv_layers': 1, 'maxpooling': 0, 'dropout_cnn': 0, 'dropout_perc_cnn': 0.11155468484884645, 'layers': 2, 'neurons': 103, 'dropout': 0, 'dropout_perc': 0.19477098919671654}\n",
      "Initializing CNNModel with kwargs: {'kernels': 16, 'kernel_size': 3, 'conv_layers': 1, 'maxpooling': 0, 'dropout_cnn': 0, 'dropout_perc_cnn': 0.11155468484884645, 'layers': 2, 'neurons': 103, 'dropout': 0, 'dropout_perc': 0.19477098919671654}\n",
      "Epoch 1, Batch 1, Loss: 0.7097\n",
      "Epoch 1, Batch 2, Loss: 39.8073\n",
      "Epoch 1, Batch 3, Loss: 38.3036\n",
      "Epoch 1, Batch 4, Loss: 36.2075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 5, Loss: 16.9147\n",
      "Epoch 1, Batch 6, Loss: 32.0559\n",
      "Epoch 1, Batch 7, Loss: 34.7612\n",
      "Epoch 1, Batch 8, Loss: 53.1262\n",
      "Epoch 1, Batch 9, Loss: 37.9858\n",
      "Epoch 1, Batch 10, Loss: 55.5311\n",
      "Epoch 1, Batch 11, Loss: 44.4776\n",
      "Epoch 1, Batch 12, Loss: 51.6574\n",
      "Epoch 1, Batch 13, Loss: 63.8893\n",
      "Epoch 2, Batch 1, Loss: 52.8100\n",
      "Epoch 2, Batch 2, Loss: 48.6331\n",
      "Epoch 2, Batch 3, Loss: 46.5444\n",
      "Epoch 2, Batch 4, Loss: 48.5164\n",
      "Epoch 2, Batch 5, Loss: 56.6502\n",
      "Epoch 2, Batch 6, Loss: 39.2954\n",
      "Epoch 2, Batch 7, Loss: 37.9493\n",
      "Epoch 2, Batch 8, Loss: 39.5769\n",
      "Epoch 2, Batch 9, Loss: 27.0082\n",
      "Epoch 2, Batch 10, Loss: 28.3219\n",
      "Epoch 2, Batch 11, Loss: 38.3831\n",
      "Epoch 2, Batch 12, Loss: 31.6910\n",
      "Epoch 2, Batch 13, Loss: 41.6667\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m2.734e+03\u001b[39m | \u001b[39m1.163    \u001b[39m | \u001b[39m0.5053   \u001b[39m | \u001b[39m0.7729   \u001b[39m | \u001b[39m0.1948   \u001b[39m | \u001b[39m0.1116   \u001b[39m | \u001b[39m3.256    \u001b[39m | \u001b[39m16.95    \u001b[39m | \u001b[39m2.217    \u001b[39m | \u001b[39m0.4944   \u001b[39m | \u001b[39m103.8    \u001b[39m |\n",
      "Calling CNNModel with kwargs: {'kernels': 51, 'kernel_size': 3, 'conv_layers': 1, 'maxpooling': 0, 'dropout_cnn': 0, 'dropout_perc_cnn': 0.23796693844063288, 'layers': 1, 'neurons': 193, 'dropout': 0, 'dropout_perc': 0.30601909521898873}\n",
      "Initializing CNNModel with kwargs: {'kernels': 51, 'kernel_size': 3, 'conv_layers': 1, 'maxpooling': 0, 'dropout_cnn': 0, 'dropout_perc_cnn': 0.23796693844063288, 'layers': 1, 'neurons': 193, 'dropout': 0, 'dropout_perc': 0.30601909521898873}\n",
      "Epoch 1, Batch 1, Loss: 0.6907\n",
      "Epoch 1, Batch 2, Loss: 42.1875\n",
      "Epoch 1, Batch 3, Loss: 41.8715\n",
      "Epoch 1, Batch 4, Loss: 32.8125\n",
      "Epoch 1, Batch 5, Loss: 34.3750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 6, Loss: 37.5000\n",
      "Epoch 1, Batch 7, Loss: 42.1875\n",
      "Epoch 1, Batch 8, Loss: 39.0625\n",
      "Epoch 1, Batch 9, Loss: 39.0625\n",
      "Epoch 1, Batch 10, Loss: 42.1875\n",
      "Epoch 1, Batch 11, Loss: 28.1250\n",
      "Epoch 1, Batch 12, Loss: 29.6875\n",
      "Epoch 1, Batch 13, Loss: 41.6667\n",
      "Epoch 2, Batch 1, Loss: 45.3125\n",
      "Epoch 2, Batch 2, Loss: 31.2500\n",
      "Epoch 2, Batch 3, Loss: 43.7500\n",
      "Epoch 2, Batch 4, Loss: 39.0625\n",
      "Epoch 2, Batch 5, Loss: 42.1875\n",
      "Epoch 2, Batch 6, Loss: 30.4585\n",
      "Epoch 2, Batch 7, Loss: 39.0625\n",
      "Epoch 2, Batch 8, Loss: 40.6250\n",
      "Epoch 2, Batch 9, Loss: 42.1875\n",
      "Epoch 2, Batch 10, Loss: 29.6875\n",
      "Epoch 2, Batch 11, Loss: 29.6875\n",
      "Epoch 2, Batch 12, Loss: 32.8125\n",
      "Epoch 2, Batch 13, Loss: 41.6667\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m2.845e+03\u001b[39m | \u001b[39m1.83     \u001b[39m | \u001b[39m0.9601   \u001b[39m | \u001b[39m0.6484   \u001b[39m | \u001b[39m0.306    \u001b[39m | \u001b[39m0.238    \u001b[39m | \u001b[39m3.787    \u001b[39m | \u001b[39m51.08    \u001b[39m | \u001b[39m1.179    \u001b[39m | \u001b[39m0.7387   \u001b[39m | \u001b[39m194.0    \u001b[39m |\n",
      "=================================================================================================================================================\n",
      "Iteration 0: \n",
      "\t{'target': 2791.7525773195875, 'params': {'conv_layers': 1.834044009405148, 'dropout': 0.7203244934421581, 'dropout_cnn': 0.00011437481734488664, 'dropout_perc': 0.22093302905273593, 'dropout_perc_cnn': 0.15870235632684523, 'kernel_size': 3.1846771895375956, 'kernels': 16.803092259904915, 'layers': 1.6911214540860955, 'maxpooling': 0.39676747423066994, 'neurons': 298.92409602546365}}\n",
      "Iteration 1: \n",
      "\t{'target': 2769.0721649484535, 'params': {'conv_layers': 1.8383890288065896, 'dropout': 0.6852195003967595, 'dropout_cnn': 0.20445224973151743, 'dropout_perc': 0.4512469745563782, 'dropout_perc_cnn': 0.11095503727917047, 'kernel_size': 4.340935020356804, 'kernels': 30.203678537293364, 'layers': 2.1173796568915035, 'maxpooling': 0.14038693859523377, 'neurons': 150.37224924100715}}\n",
      "Iteration 2: \n",
      "\t{'target': 2626.8041237113403, 'params': {'conv_layers': 2.6014891373510736, 'dropout': 0.9682615757193975, 'dropout_cnn': 0.31342417815924284, 'dropout_perc': 0.3769290462677256, 'dropout_perc_cnn': 0.45055566091841537, 'kernel_size': 4.789213327007695, 'kernels': 10.932564259447119, 'layers': 1.0781095664657647, 'maxpooling': 0.1698304195645689, 'neurons': 446.87013149522414}}\n",
      "Iteration 3: \n",
      "\t{'target': 2742.2680412371133, 'params': {'conv_layers': 1.1966936676661002, 'dropout': 0.42110762500505217, 'dropout_cnn': 0.9578895301505019, 'dropout_perc': 0.31326611398920684, 'dropout_perc_cnn': 0.37675084558018934, 'kernel_size': 3.631031262012126, 'kernels': 45.817053805531856, 'layers': 2.6692513437947456, 'maxpooling': 0.018288277344191806, 'neurons': 391.0629213160058}}\n",
      "Iteration 4: \n",
      "\t{'target': 2503.092783505155, 'params': {'conv_layers': 2.977722177812989, 'dropout': 0.7481656543798394, 'dropout_cnn': 0.2804439920644052, 'dropout_perc': 0.4157117313805955, 'dropout_perc_cnn': 0.14129040263105683, 'kernel_size': 3.8957870523518103, 'kernels': 58.69853917939954, 'layers': 1.587228296747359, 'maxpooling': 0.28777533858634874, 'neurons': 120.69245744356907}}\n",
      "Iteration 5: \n",
      "\t{'target': 2880.4123711340208, 'params': {'conv_layers': 2.347788606119317, 'dropout': 0.8896824089806917, 'dropout_cnn': 0.42926838533903355, 'dropout_perc': 0.3811236937722066, 'dropout_perc_cnn': 0.12673324992230403, 'kernel_size': 3.3716661100785834, 'kernels': 16.27615574508908, 'layers': 1.0780624499560587, 'maxpooling': 0.3834324365501866, 'neurons': 298.1651658603215}}\n",
      "Iteration 6: \n",
      "\t{'target': 2720.618556701031, 'params': {'conv_layers': 3.0, 'dropout': 1.0, 'dropout_cnn': 1.0, 'dropout_perc': 0.5, 'dropout_perc_cnn': 0.1, 'kernel_size': 3.7996117587171505, 'kernels': 15.069921705465188, 'layers': 1.0, 'maxpooling': 0.3523523102708798, 'neurons': 296.43900711344406}}\n",
      "Iteration 7: \n",
      "\t{'target': 2734.020618556701, 'params': {'conv_layers': 3.0, 'dropout': 1.0, 'dropout_cnn': 1.0, 'dropout_perc': 0.5, 'dropout_perc_cnn': 0.1, 'kernel_size': 3.0795553799766, 'kernels': 17.44081828653327, 'layers': 1.0, 'maxpooling': 0.725941182835325, 'neurons': 297.91961207757987}}\n",
      "Iteration 8: \n",
      "\t{'target': 2553.6082474226805, 'params': {'conv_layers': 2.2865372080775965, 'dropout': 1.0, 'dropout_cnn': 0.30750922544517506, 'dropout_perc': 0.5, 'dropout_perc_cnn': 0.1, 'kernel_size': 3.572952942535616, 'kernels': 15.352694538011463, 'layers': 1.0, 'maxpooling': 0.03369397266452406, 'neurons': 298.8221626349498}}\n",
      "Iteration 9: \n",
      "\t{'target': 2734.020618556701, 'params': {'conv_layers': 1.1627829237240253, 'dropout': 0.5052670551266611, 'dropout_cnn': 0.7728887589336584, 'dropout_perc': 0.19477098919671654, 'dropout_perc_cnn': 0.11155468484884645, 'kernel_size': 3.255971014390403, 'kernels': 16.94639970029877, 'layers': 2.2168392955244314, 'maxpooling': 0.4944288158081669, 'neurons': 103.79067916711844}}\n",
      "Iteration 10: \n",
      "\t{'target': 2845.360824742268, 'params': {'conv_layers': 1.8302658311069144, 'dropout': 0.9600626468976723, 'dropout_cnn': 0.6484095731765869, 'dropout_perc': 0.30601909521898873, 'dropout_perc_cnn': 0.23796693844063288, 'kernel_size': 3.787264339041474, 'kernels': 51.08013008533003, 'layers': 1.178904019693525, 'maxpooling': 0.7386627689015391, 'neurons': 193.98853135046852}}\n",
      "{'target': 2880.4123711340208, 'params': {'conv_layers': 2.347788606119317, 'dropout': 0.8896824089806917, 'dropout_cnn': 0.42926838533903355, 'dropout_perc': 0.3811236937722066, 'dropout_perc_cnn': 0.12673324992230403, 'kernel_size': 3.3716661100785834, 'kernels': 16.27615574508908, 'layers': 1.0780624499560587, 'maxpooling': 0.3834324365501866, 'neurons': 298.1651658603215}}\n"
     ]
    }
   ],
   "source": [
    "%pip install bayesian_optimization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, image_shape, **kwargs):\n",
    "        super(CNNModel, self).__init__()\n",
    "        layers = []\n",
    "        print(f\"Initializing CNNModel with kwargs: {kwargs}\")\n",
    "        layers.append(nn.Conv2d(3, kwargs['kernels'], kernel_size=(kwargs['kernel_size'], kwargs['kernel_size']), groups=1))\n",
    "        for _ in range(kwargs['conv_layers']):\n",
    "            layers.append(nn.Conv2d(kwargs['kernels'], kwargs['kernels'], kernel_size=(kwargs['kernel_size'], kwargs['kernel_size']), groups=1))\n",
    "            if kwargs['maxpooling'] == 1:\n",
    "                layers.append(nn.MaxPool2d(kernel_size=(2, 2)))\n",
    "            if kwargs['dropout_cnn'] == 1:\n",
    "                layers.append(nn.Dropout(kwargs['dropout_perc_cnn']))\n",
    "        layers.append(nn.Flatten())\n",
    "        \n",
    "        # Calculate the size of the flattened layer\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *image_shape)\n",
    "            dummy_output = nn.Sequential(*layers)(dummy_input)\n",
    "            flattened_size = dummy_output.numel()\n",
    "        \n",
    "        for _ in range(kwargs['layers']):\n",
    "            layers.append(nn.Linear(flattened_size if _ == 0 else kwargs['neurons'], kwargs['neurons']))\n",
    "            if kwargs['dropout'] == 1:\n",
    "                layers.append(nn.Dropout(kwargs['dropout_perc']))\n",
    "        layers.append(nn.Linear(kwargs['neurons'], 1))  # Binary classification\n",
    "        layers.append(nn.Sigmoid())  # Use sigmoid activation\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def fit_with(kernels, kernel_size, conv_layers, maxpooling, dropout_cnn, dropout_perc_cnn, layers, neurons, dropout, dropout_perc, input_shape=None, verbose=1):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "    kwargs = {\n",
    "        'kernels': int(kernels),\n",
    "        'kernel_size': int(kernel_size),\n",
    "        'conv_layers': int(conv_layers),\n",
    "        'maxpooling': int(maxpooling),\n",
    "        'dropout_cnn': int(dropout_cnn),\n",
    "        'dropout_perc_cnn': float(dropout_perc_cnn),\n",
    "        'layers': int(layers),\n",
    "        'neurons': int(neurons),\n",
    "        'dropout': int(dropout),\n",
    "        'dropout_perc': float(dropout_perc)\n",
    "    }\n",
    "    print(f\"Calling CNNModel with kwargs: {kwargs}\")\n",
    "    net = CNNModel(input_shape[1:], **kwargs)  # Adjust input_shape to remove the batch dimension\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "   \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        model.train()\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        train_loss, train_correct = 0, 0\n",
    "\n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            X = X.squeeze(0)  # Remove the extra dimension\n",
    "            y_pred = model(X).squeeze()\n",
    "            loss = loss_fn(y_pred, y.float())\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            y_pred_class = (y_pred > 0.5).float()\n",
    "            train_correct += (y_pred_class == y).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        train_acc = train_correct / len(train_loader.dataset)\n",
    "        train_accuracies.append(train_acc)\n",
    "        \n",
    "    \n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = net(inputs)\n",
    "            predicted = (outputs > 0.5).float()  # Convert to binary predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.float()).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    #print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "input_shape = img.shape  # img.shape = [4, 3, 224, 224]\n",
    "verbose = 1\n",
    "fit_with_partial = partial(fit_with, input_shape=input_shape, verbose=verbose)\n",
    "pbounds = {\n",
    "    'kernels': (6, 64),\n",
    "    'kernel_size': (3, 5),\n",
    "    'conv_layers': (1, 3),\n",
    "    'maxpooling': (0, 1),\n",
    "    'dropout_cnn': (0, 1),\n",
    "    'dropout_perc_cnn': (0.1, 0.5),\n",
    "    'layers': (1, 3),\n",
    "    'neurons': (64, 500),\n",
    "    'dropout': (0, 1),\n",
    "    'dropout_perc': (0.1, 0.5)\n",
    "}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=fit_with_partial,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,\n",
    "    random_state=1,\n",
    ")\n",
    "optimizer.maximize(init_points=5, n_iter=6)\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "    \n",
    "#print the result of the best model\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e71f9d-20e3-40a3-b1bf-0e8f50e031b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
